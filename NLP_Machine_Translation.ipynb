{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Machine Translation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rigved19/NLP-Language-Translation/blob/main/NLP_Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8Am9D-ZGrg5"
      },
      "source": [
        "###Cleaning and Text-Preprocessing \n",
        "1. Adding additional tokens <sos> and <eos> for the German Sentences\n",
        "2. Lowercasing\n",
        "3. Removing Punctuations\n",
        "4. Remove words with numbers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9_xvTJ6QnTb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f5b1e8f-8645-4894-8285-97ded45938bd"
      },
      "source": [
        "import re\n",
        "import copy\n",
        "import pickle\n",
        "from matplotlib import pyplot as plt\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "import string \n",
        "import numpy as np\n",
        "from numpy import asarray\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from numpy import zeros\n",
        " \n",
        "#Cleaning and Processing Data (English and German sentences)\n",
        "input_sentences = []\n",
        "output_sentences = []\n",
        "output_sentences_inputs = []\n",
        "clean_input_sentences = []\n",
        "clean_output_sentences = []\n",
        "clean_output_sentences_inputs = []\n",
        " \n",
        "NUM_SENTENCES =1000   #Taking the first 1000 sentences as our examples\n",
        "count = 0\n",
        " \n",
        "#Adding <eos> and <sos> for corresponding output sequences\n",
        "#<sos> is \"start of sentence\" and <eos> is \"end of sentence\" for Output Sequences\n",
        "for line in open(r'/content/deu.txt', encoding=\"utf-8\"):\n",
        "    count += 1\n",
        " \n",
        "    if count > NUM_SENTENCES:\n",
        "        break\n",
        " \n",
        "    input_sentence, output , extra = line.rstrip().split('\\t')\n",
        "    \n",
        "    output_sentence = output + ' <eos>'\n",
        "    output_sentence_input = '<sos> ' + output\n",
        " \n",
        "    input_sentences.append(input_sentence)\n",
        "    output_sentences.append(output_sentence)\n",
        "    output_sentences_inputs.append(output_sentence_input)\n",
        " \n",
        "\n",
        "#Cleaning all the data,seperating and storing\n",
        "def clean_pairs(lines):\n",
        "    cleaned = list()\n",
        "    # prepare translation table for removing punctuation\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    for line in lines:\n",
        "        clean_pair = list()\n",
        "        # tokenize on white space\n",
        "        line = line.split()\n",
        "        # convert to lowercase\n",
        "        line = [word.lower() for word in line]\n",
        "        # remove punctuation from each token\n",
        "        line = [word.translate(table) for word in line]\n",
        "        # remove tokens with numbers in them\n",
        "        line = [word for word in line if word.isalpha()]\n",
        "        # store as string\n",
        "        clean_pair.append(' '.join(line))\n",
        "        cleaned.append(clean_pair)\n",
        "    return array(cleaned)\n",
        " \n",
        "clean_input_sentences =  clean_pairs(input_sentences)\n",
        "clean_output_sentences = clean_pairs(output_sentences)\n",
        "clean_output_sentences_inputs = clean_pairs(output_sentences_inputs)\n",
        " \n",
        "print(\"Printing processed English Sentences : \"  , clean_input_sentences[959])\n",
        "print(\"Printing corresponding processed Gemran Input Sentences : \" ,  clean_output_sentences_inputs[959])\n",
        "print(\"Printing Gemran Output Sentences\" , clean_output_sentences[959])\n",
        "\n",
        "#Counting no. of sentences(examples) in \n",
        "input_examples = len(clean_input_sentences)\n",
        "output_examples = len(clean_output_sentences)\n",
        "\n",
        "print(\"\\nNumber if English Sentences\" , input_examples)\n",
        "print(\"Number if German Sentences\" , output_examples)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Printing processed English Sentences :  ['keep them']\n",
            "Printing corresponding processed Gemran Input Sentences :  ['sos behalt sie']\n",
            "Printing Gemran Output Sentences ['behalt sie eos']\n",
            "\n",
            "Number if English Sentences 1000\n",
            "Number if German Sentences 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygRAWnF_Sn2O"
      },
      "source": [
        "###Creating dictionaries for both languages\n",
        "*   Word to Index\n",
        "*   Index to word\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBvMNYGAQwG6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96d7c27b-cee2-4ac3-b245-f6598b1009f9"
      },
      "source": [
        "#Creating dictionary for English Vob and German Vocab\n",
        "#Contains a dictionary with unique words as keys with index numbers as values\n",
        "\n",
        "def make_dict(sentences):\n",
        "  set_input = set()              #Set() helps to count multiple same words only once\n",
        "  dict_vocab = {}\n",
        "  dict_index = {}\n",
        "\n",
        "  for i in sentences:\n",
        "    for j in i:\n",
        "      words = j.split()\n",
        "      set_input.update(words) \n",
        "  \n",
        "  list_input = sorted(list(set_input))  # .sorted() - Sorting the list of Words in Ascending order\n",
        "  vocab_size = len(list_input)\n",
        "  \n",
        "\n",
        "  for i in range(vocab_size):         #Forming Vocab Dictictionaries ( Words --> index)\n",
        "    dict_vocab[list_input[i]] = i+1   # i+1 so that it can be read easier , because of 0 index\n",
        "\n",
        "  for i in range(vocab_size):         #Forming Vocab Dictictionaries ( index --> Words)\n",
        "    dict_index[i+1] = list_input[i]\n",
        " \n",
        "\n",
        "  return(vocab_size,dict_vocab,dict_index)\n",
        "\n",
        "#Forming English/German ( Vocab, Index, Size)\n",
        "vocab_eng_size , vocab_eng , index_eng = make_dict(clean_input_sentences)\n",
        "vocab_german_size, vocab_german , index_german = make_dict(clean_output_sentences)\n",
        "\n",
        "#Adding the extra word \"sos\" in the german dictionary\n",
        "vocab_german_size = vocab_german_size + 1\n",
        "vocab_german[\"sos\"] = vocab_german_size\n",
        "index_german[vocab_german_size] = \"sos\"\n",
        "\n",
        "\n",
        "print(\"\\nEnlish Word-->Index :\",vocab_eng)\n",
        "print(\"\\nEnlish Index-->Word :\",index_eng)\n",
        "print(\"\\nGerman Word-->Index :\",vocab_german)\n",
        "print(\"\\nGerman Index-->Word :\",index_german)\n",
        "print(\"\\nEng Dictionary Size: \",vocab_eng_size)\n",
        "print(\"German Dictionary Size:\",vocab_german_size)\n",
        "idx_sos = vocab_german[\"sos\"]                      #Storing <sos> token number, used for sampling test cases\n",
        "idx_eos = vocab_german[\"eos\"]                      #Storing <eos> token number, used for sampling test cases\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Enlish Word-->Index : {'a': 1, 'agree': 2, 'ahead': 3, 'aim': 4, 'all': 5, 'alone': 6, 'am': 7, 'angry': 8, 'answer': 9, 'approve': 10, 'ask': 11, 'ate': 12, 'attack': 13, 'awake': 14, 'away': 15, 'awesome': 16, 'awful': 17, 'back': 18, 'bad': 19, 'bald': 20, 'bark': 21, 'be': 22, 'beat': 23, 'bed': 24, 'begin': 25, 'birds': 26, 'bless': 27, 'blind': 28, 'bored': 29, 'brave': 30, 'brief': 31, 'broke': 32, 'burn': 33, 'burned': 34, 'bury': 35, 'busy': 36, 'buy': 37, 'by': 38, 'call': 39, 'calm': 40, 'came': 41, 'can': 42, 'care': 43, 'catch': 44, 'changed': 45, 'cheated': 46, 'cheer': 47, 'cheers': 48, 'chill': 49, 'clapped': 50, 'cold': 51, 'come': 52, 'cook': 53, 'cool': 54, 'coughed': 55, 'crazy': 56, 'cried': 57, 'cry': 58, 'cuff': 59, 'cured': 60, 'cursed': 61, 'cute': 62, 'dark': 63, 'dead': 64, 'deaf': 65, 'deep': 66, 'did': 67, 'die': 68, 'died': 69, 'dj': 70, 'do': 71, 'dogs': 72, 'dont': 73, 'down': 74, 'drive': 75, 'drop': 76, 'drove': 77, 'drunk': 78, 'dry': 79, 'duck': 80, 'dying': 81, 'early': 82, 'east': 83, 'easy': 84, 'eat': 85, 'escaped': 86, 'excuse': 87, 'fail': 88, 'fainted': 89, 'fair': 90, 'fantastic': 91, 'far': 92, 'fast': 93, 'fat': 94, 'fear': 95, 'feel': 96, 'fell': 97, 'find': 98, 'fine': 99, 'fire': 100, 'first': 101, 'fix': 102, 'fled': 103, 'fly': 104, 'fold': 105, 'follow': 106, 'for': 107, 'forget': 108, 'free': 109, 'freeze': 110, 'frowned': 111, 'froze': 112, 'full': 113, 'fun': 114, 'fussy': 115, 'game': 116, 'gave': 117, 'get': 118, 'giggled': 119, 'give': 120, 'go': 121, 'going': 122, 'gone': 123, 'good': 124, 'goodbye': 125, 'goodnight': 126, 'got': 127, 'grab': 128, 'had': 129, 'hands': 130, 'hang': 131, 'happy': 132, 'hard': 133, 'have': 134, 'he': 135, 'head': 136, 'hello': 137, 'help': 138, 'helped': 139, 'helps': 140, 'here': 141, 'heres': 142, 'hes': 143, 'hi': 144, 'hid': 145, 'him': 146, 'his': 147, 'hit': 148, 'hold': 149, 'home': 150, 'hop': 151, 'hope': 152, 'hot': 153, 'how': 154, 'hug': 155, 'humor': 156, 'hung': 157, 'hurried': 158, 'hurry': 159, 'hurts': 160, 'i': 161, 'id': 162, 'ignore': 163, 'ill': 164, 'im': 165, 'in': 166, 'inside': 167, 'is': 168, 'it': 169, 'its': 170, 'ive': 171, 'join': 172, 'jump': 173, 'jumped': 174, 'just': 175, 'keep': 176, 'kill': 177, 'kind': 178, 'kiss': 179, 'knew': 180, 'know': 181, 'late': 182, 'laughed': 183, 'lazy': 184, 'leave': 185, 'left': 186, 'let': 187, 'lets': 188, 'lie': 189, 'lied': 190, 'lies': 191, 'like': 192, 'listen': 193, 'live': 194, 'lock': 195, 'long': 196, 'look': 197, 'loosen': 198, 'lost': 199, 'love': 200, 'low': 201, 'lying': 202, 'mad': 203, 'made': 204, 'man': 205, 'marry': 206, 'may': 207, 'me': 208, 'mean': 209, 'met': 210, 'mine': 211, 'miss': 212, 'moaned': 213, 'move': 214, 'needy': 215, 'new': 216, 'next': 217, 'nice': 218, 'no': 219, 'nodded': 220, 'now': 221, 'obese': 222, 'obey': 223, 'off': 224, 'ok': 225, 'okay': 226, 'old': 227, 'on': 228, 'one': 229, 'open': 230, 'ours': 231, 'out': 232, 'over': 233, 'paid': 234, 'paused': 235, 'pay': 236, 'perfect': 237, 'picky': 238, 'poured': 239, 'prayed': 240, 'promise': 241, 'pull': 242, 'push': 243, 'quit': 244, 'ran': 245, 'ready': 246, 'real': 247, 'really': 248, 'red': 249, 'refuse': 250, 'relax': 251, 'relaxed': 252, 'resign': 253, 'rested': 254, 'retired': 255, 'rich': 256, 'right': 257, 'rise': 258, 'rude': 259, 'run': 260, 'runs': 261, 'sad': 262, 'safe': 263, 'said': 264, 'sand': 265, 'sang': 266, 'save': 267, 'saw': 268, 'seated': 269, 'see': 270, 'sexy': 271, 'shaved': 272, 'she': 273, 'shoot': 274, 'show': 275, 'shut': 276, 'shy': 277, 'sick': 278, 'sing': 279, 'sit': 280, 'ski': 281, 'slipped': 282, 'smile': 283, 'smiled': 284, 'snore': 285, 'snowed': 286, 'so': 287, 'sober': 288, 'some': 289, 'soon': 290, 'sorry': 291, 'speak': 292, 'sped': 293, 'speed': 294, 'spit': 295, 'spoke': 296, 'spy': 297, 'stand': 298, 'stay': 299, 'stayed': 300, 'still': 301, 'stinks': 302, 'stop': 303, 'stopped': 304, 'stuck': 305, 'sure': 306, 'swam': 307, 'swim': 308, 'swore': 309, 'take': 310, 'talk': 311, 'tall': 312, 'taste': 313, 'tea': 314, 'tell': 315, 'terrific': 316, 'thanks': 317, 'that': 318, 'them': 319, 'they': 320, 'thin': 321, 'this': 322, 'tidy': 323, 'tired': 324, 'to': 325, 'tom': 326, 'toms': 327, 'too': 328, 'took': 329, 'touch': 330, 'tough': 331, 'tried': 332, 'tries': 333, 'true': 334, 'trust': 335, 'try': 336, 'tv': 337, 'ugly': 338, 'up': 339, 'upset': 340, 'us': 341, 'use': 342, 'wait': 343, 'waited': 344, 'wake': 345, 'walk': 346, 'want': 347, 'warm': 348, 'warn': 349, 'was': 350, 'wash': 351, 'watch': 352, 'waved': 353, 'way': 354, 'we': 355, 'weak': 356, 'weird': 357, 'welcome': 358, 'well': 359, 'went': 360, 'wept': 361, 'were': 362, 'wet': 363, 'what': 364, 'who': 365, 'whos': 366, 'why': 367, 'will': 368, 'win': 369, 'winked': 370, 'woke': 371, 'won': 372, 'work': 373, 'worked': 374, 'works': 375, 'wow': 376, 'write': 377, 'yawned': 378, 'you': 379, 'yours': 380}\n",
            "\n",
            "Enlish Index-->Word : {1: 'a', 2: 'agree', 3: 'ahead', 4: 'aim', 5: 'all', 6: 'alone', 7: 'am', 8: 'angry', 9: 'answer', 10: 'approve', 11: 'ask', 12: 'ate', 13: 'attack', 14: 'awake', 15: 'away', 16: 'awesome', 17: 'awful', 18: 'back', 19: 'bad', 20: 'bald', 21: 'bark', 22: 'be', 23: 'beat', 24: 'bed', 25: 'begin', 26: 'birds', 27: 'bless', 28: 'blind', 29: 'bored', 30: 'brave', 31: 'brief', 32: 'broke', 33: 'burn', 34: 'burned', 35: 'bury', 36: 'busy', 37: 'buy', 38: 'by', 39: 'call', 40: 'calm', 41: 'came', 42: 'can', 43: 'care', 44: 'catch', 45: 'changed', 46: 'cheated', 47: 'cheer', 48: 'cheers', 49: 'chill', 50: 'clapped', 51: 'cold', 52: 'come', 53: 'cook', 54: 'cool', 55: 'coughed', 56: 'crazy', 57: 'cried', 58: 'cry', 59: 'cuff', 60: 'cured', 61: 'cursed', 62: 'cute', 63: 'dark', 64: 'dead', 65: 'deaf', 66: 'deep', 67: 'did', 68: 'die', 69: 'died', 70: 'dj', 71: 'do', 72: 'dogs', 73: 'dont', 74: 'down', 75: 'drive', 76: 'drop', 77: 'drove', 78: 'drunk', 79: 'dry', 80: 'duck', 81: 'dying', 82: 'early', 83: 'east', 84: 'easy', 85: 'eat', 86: 'escaped', 87: 'excuse', 88: 'fail', 89: 'fainted', 90: 'fair', 91: 'fantastic', 92: 'far', 93: 'fast', 94: 'fat', 95: 'fear', 96: 'feel', 97: 'fell', 98: 'find', 99: 'fine', 100: 'fire', 101: 'first', 102: 'fix', 103: 'fled', 104: 'fly', 105: 'fold', 106: 'follow', 107: 'for', 108: 'forget', 109: 'free', 110: 'freeze', 111: 'frowned', 112: 'froze', 113: 'full', 114: 'fun', 115: 'fussy', 116: 'game', 117: 'gave', 118: 'get', 119: 'giggled', 120: 'give', 121: 'go', 122: 'going', 123: 'gone', 124: 'good', 125: 'goodbye', 126: 'goodnight', 127: 'got', 128: 'grab', 129: 'had', 130: 'hands', 131: 'hang', 132: 'happy', 133: 'hard', 134: 'have', 135: 'he', 136: 'head', 137: 'hello', 138: 'help', 139: 'helped', 140: 'helps', 141: 'here', 142: 'heres', 143: 'hes', 144: 'hi', 145: 'hid', 146: 'him', 147: 'his', 148: 'hit', 149: 'hold', 150: 'home', 151: 'hop', 152: 'hope', 153: 'hot', 154: 'how', 155: 'hug', 156: 'humor', 157: 'hung', 158: 'hurried', 159: 'hurry', 160: 'hurts', 161: 'i', 162: 'id', 163: 'ignore', 164: 'ill', 165: 'im', 166: 'in', 167: 'inside', 168: 'is', 169: 'it', 170: 'its', 171: 'ive', 172: 'join', 173: 'jump', 174: 'jumped', 175: 'just', 176: 'keep', 177: 'kill', 178: 'kind', 179: 'kiss', 180: 'knew', 181: 'know', 182: 'late', 183: 'laughed', 184: 'lazy', 185: 'leave', 186: 'left', 187: 'let', 188: 'lets', 189: 'lie', 190: 'lied', 191: 'lies', 192: 'like', 193: 'listen', 194: 'live', 195: 'lock', 196: 'long', 197: 'look', 198: 'loosen', 199: 'lost', 200: 'love', 201: 'low', 202: 'lying', 203: 'mad', 204: 'made', 205: 'man', 206: 'marry', 207: 'may', 208: 'me', 209: 'mean', 210: 'met', 211: 'mine', 212: 'miss', 213: 'moaned', 214: 'move', 215: 'needy', 216: 'new', 217: 'next', 218: 'nice', 219: 'no', 220: 'nodded', 221: 'now', 222: 'obese', 223: 'obey', 224: 'off', 225: 'ok', 226: 'okay', 227: 'old', 228: 'on', 229: 'one', 230: 'open', 231: 'ours', 232: 'out', 233: 'over', 234: 'paid', 235: 'paused', 236: 'pay', 237: 'perfect', 238: 'picky', 239: 'poured', 240: 'prayed', 241: 'promise', 242: 'pull', 243: 'push', 244: 'quit', 245: 'ran', 246: 'ready', 247: 'real', 248: 'really', 249: 'red', 250: 'refuse', 251: 'relax', 252: 'relaxed', 253: 'resign', 254: 'rested', 255: 'retired', 256: 'rich', 257: 'right', 258: 'rise', 259: 'rude', 260: 'run', 261: 'runs', 262: 'sad', 263: 'safe', 264: 'said', 265: 'sand', 266: 'sang', 267: 'save', 268: 'saw', 269: 'seated', 270: 'see', 271: 'sexy', 272: 'shaved', 273: 'she', 274: 'shoot', 275: 'show', 276: 'shut', 277: 'shy', 278: 'sick', 279: 'sing', 280: 'sit', 281: 'ski', 282: 'slipped', 283: 'smile', 284: 'smiled', 285: 'snore', 286: 'snowed', 287: 'so', 288: 'sober', 289: 'some', 290: 'soon', 291: 'sorry', 292: 'speak', 293: 'sped', 294: 'speed', 295: 'spit', 296: 'spoke', 297: 'spy', 298: 'stand', 299: 'stay', 300: 'stayed', 301: 'still', 302: 'stinks', 303: 'stop', 304: 'stopped', 305: 'stuck', 306: 'sure', 307: 'swam', 308: 'swim', 309: 'swore', 310: 'take', 311: 'talk', 312: 'tall', 313: 'taste', 314: 'tea', 315: 'tell', 316: 'terrific', 317: 'thanks', 318: 'that', 319: 'them', 320: 'they', 321: 'thin', 322: 'this', 323: 'tidy', 324: 'tired', 325: 'to', 326: 'tom', 327: 'toms', 328: 'too', 329: 'took', 330: 'touch', 331: 'tough', 332: 'tried', 333: 'tries', 334: 'true', 335: 'trust', 336: 'try', 337: 'tv', 338: 'ugly', 339: 'up', 340: 'upset', 341: 'us', 342: 'use', 343: 'wait', 344: 'waited', 345: 'wake', 346: 'walk', 347: 'want', 348: 'warm', 349: 'warn', 350: 'was', 351: 'wash', 352: 'watch', 353: 'waved', 354: 'way', 355: 'we', 356: 'weak', 357: 'weird', 358: 'welcome', 359: 'well', 360: 'went', 361: 'wept', 362: 'were', 363: 'wet', 364: 'what', 365: 'who', 366: 'whos', 367: 'why', 368: 'will', 369: 'win', 370: 'winked', 371: 'woke', 372: 'won', 373: 'work', 374: 'worked', 375: 'works', 376: 'wow', 377: 'write', 378: 'yawned', 379: 'you', 380: 'yours'}\n",
            "\n",
            "German Word-->Index : {'ab': 1, 'abbrechen': 2, 'abend': 3, 'acker': 4, 'aha': 5, 'alle': 6, 'allein': 7, 'alt': 8, 'amüsiert': 9, 'an': 10, 'angriff': 11, 'anhalten': 12, 'anrufen': 13, 'antworten': 14, 'arbeit': 15, 'arm': 16, 'attacke': 17, 'auch': 18, 'auf': 19, 'aufgegeben': 20, 'aufgehört': 21, 'aufgelegt': 22, 'aufhören': 23, 'aufstehen': 24, 'aus': 25, 'ausgerutscht': 26, 'ausgeschieden': 27, 'ausgeschlossen': 28, 'aß': 29, 'bald': 30, 'beachte': 31, 'beachten': 32, 'bedürftig': 33, 'beeil': 34, 'beeilt': 35, 'begegnet': 36, 'behalt': 37, 'behalte': 38, 'behalten': 39, 'behaltet': 40, 'behebe': 41, 'beheben': 42, 'bei': 43, 'bekommen': 44, 'bellen': 45, 'benutze': 46, 'beobachte': 47, 'beobachten': 48, 'beobachtet': 49, 'bereit': 50, 'beruhige': 51, 'beruhigen': 52, 'bescheid': 53, 'beschleunigte': 54, 'beschäftigt': 55, 'bestürzt': 56, 'betete': 57, 'betrunken': 58, 'bett': 59, 'beweg': 60, 'bewegen': 61, 'bewegung': 62, 'bezahlt': 63, 'bin': 64, 'bis': 65, 'blau': 66, 'bleib': 67, 'bleiben': 68, 'bleibt': 69, 'blieb': 70, 'blind': 71, 'blinzelte': 72, 'brannte': 73, 'bring': 74, 'bringen': 75, 'bringt': 76, 'böse': 77, 'cool': 78, 'da': 79, 'dabei': 80, 'dageblieben': 81, 'danke': 82, 'daraus': 83, 'darf': 84, 'das': 85, 'davon': 86, 'deckung': 87, 'den': 88, 'der': 89, 'dich': 90, 'dick': 91, 'die': 92, 'dir': 93, 'dj': 94, 'doch': 95, 'dollar': 96, 'donnerwetter': 97, 'dran': 98, 'drauf': 99, 'dreh': 100, 'drück': 101, 'drücken': 102, 'drückt': 103, 'du': 104, 'dunkel': 105, 'dünn': 106, 'echt': 107, 'ein': 108, 'eine': 109, 'einen': 110, 'einfach': 111, 'eintritt': 112, 'einverstanden': 113, 'entkam': 114, 'entkommen': 115, 'entschuldigen': 116, 'entschuldigung': 117, 'entspann': 118, 'entspanne': 119, 'entspannt': 120, 'eos': 121, 'er': 122, 'ernst': 123, 'ernsthaft': 124, 'erste': 125, 'erster': 126, 'erzähl': 127, 'erzählen': 128, 'erzähls': 129, 'es': 130, 'essen': 131, 'etwas': 132, 'euch': 133, 'fahr': 134, 'fahren': 135, 'fahrt': 136, 'fair': 137, 'fallen': 138, 'falte': 139, 'fang': 140, 'fange': 141, 'fangen': 142, 'fangt': 143, 'fantastisch': 144, 'fass': 145, 'fassen': 146, 'fasst': 147, 'faul': 148, 'fehlt': 149, 'feier': 150, 'fernseher': 151, 'fertig': 152, 'fest': 153, 'fett': 154, 'feuer': 155, 'fiel': 156, 'finde': 157, 'finden': 158, 'findet': 159, 'fliege': 160, 'fliegen': 161, 'fluchte': 162, 'flüchtete': 163, 'folge': 164, 'fort': 165, 'frag': 166, 'frage': 167, 'fragen': 168, 'fragt': 169, 'frei': 170, 'froh': 171, 'fror': 172, 'früh': 173, 'fuhr': 174, 'funktioniert': 175, 'fuß': 176, 'füge': 177, 'fühl': 178, 'fünf': 179, 'fünfundvierzig': 180, 'für': 181, 'fürchte': 182, 'gab': 183, 'ganz': 184, 'gar': 185, 'gebe': 186, 'gefahren': 187, 'gefallen': 188, 'geflucht': 189, 'geflüchtet': 190, 'gefroren': 191, 'gefällt': 192, 'gegangen': 193, 'gegessen': 194, 'gegähnt': 195, 'geh': 196, 'gehe': 197, 'geheilt': 198, 'gehen': 199, 'geholfen': 200, 'gehorchen': 201, 'geht': 202, 'gehts': 203, 'gehustet': 204, 'gehöre': 205, 'gehörlos': 206, 'gehört': 207, 'geklappt': 208, 'gekommen': 209, 'gekündigt': 210, 'gelangweilt': 211, 'gelassen': 212, 'gelogen': 213, 'gemacht': 214, 'gemein': 215, 'gemeint': 216, 'genommen': 217, 'gerannt': 218, 'gerecht': 219, 'geschafft': 220, 'geschlagen': 221, 'geschneit': 222, 'geschummelt': 223, 'geschwommen': 224, 'geschworen': 225, 'gesehen': 226, 'gesicht': 227, 'gesprungen': 228, 'gespuckt': 229, 'gestorben': 230, 'gestürzt': 231, 'gesundheit': 232, 'getroffen': 233, 'gewartet': 234, 'geweint': 235, 'gewinne': 236, 'gewinnen': 237, 'gewonnen': 238, 'gewunken': 239, 'gewusst': 240, 'gezahlt': 241, 'geöffnet': 242, 'ging': 243, 'glatze': 244, 'glatzköpfig': 245, 'glücklich': 246, 'gott': 247, 'greif': 248, 'groß': 249, 'grüß': 250, 'guck': 251, 'gut': 252, 'gute': 253, 'gähnte': 254, 'hab': 255, 'habe': 256, 'haben': 257, 'habs': 258, 'habt': 259, 'halb': 260, 'half': 261, 'hallo': 262, 'halt': 263, 'halte': 264, 'halten': 265, 'haltet': 266, 'handschellen': 267, 'hart': 268, 'hast': 269, 'hat': 270, 'hatte': 271, 'hau': 272, 'hause': 273, 'heim': 274, 'heirate': 275, 'heiß': 276, 'helfe': 277, 'helfen': 278, 'helft': 279, 'her': 280, 'herein': 281, 'hervorragend': 282, 'hielt': 283, 'hier': 284, 'hierher': 285, 'hilf': 286, 'hilfe': 287, 'hilft': 288, 'hin': 289, 'hingefallen': 290, 'hinlegen': 291, 'hinten': 292, 'hinter': 293, 'hoch': 294, 'hoffe': 295, 'hol': 296, 'hols': 297, 'hunde': 298, 'hustete': 299, 'hände': 300, 'hässlich': 301, 'hör': 302, 'hört': 303, 'hörte': 304, 'hülf': 305, 'ich': 306, 'ihm': 307, 'ihn': 308, 'ihr': 309, 'im': 310, 'in': 311, 'ins': 312, 'iss': 313, 'ist': 314, 'ja': 315, 'jahre': 316, 'jetzt': 317, 'kalt': 318, 'kam': 319, 'kann': 320, 'kannst': 321, 'kapiert': 322, 'kasse': 323, 'kein': 324, 'keine': 325, 'keinster': 326, 'kicherte': 327, 'klappt': 328, 'klar': 329, 'klatschte': 330, 'knapp': 331, 'knicken': 332, 'kochen': 333, 'komm': 334, 'komme': 335, 'kommen': 336, 'kommt': 337, 'kopf': 338, 'krank': 339, 'kurz': 340, 'können': 341, 'kühl': 342, 'küsse': 343, 'küssen': 344, 'küsst': 345, 'lachte': 346, 'lage': 347, 'langweilig': 348, 'lass': 349, 'lassen': 350, 'lasst': 351, 'lauf': 352, 'laufen': 353, 'lauter': 354, 'leb': 355, 'leben': 356, 'leg': 357, 'legen': 358, 'legt': 359, 'leid': 360, 'leine': 361, 'liebe': 362, 'lief': 363, 'lieg': 364, 'locker': 365, 'log': 366, 'losgehen': 367, 'lächeln': 368, 'lächelte': 369, 'läufst': 370, 'läuft': 371, 'lüge': 372, 'lügt': 373, 'mach': 374, 'mache': 375, 'macht': 376, 'mal': 377, 'mann': 378, 'maul': 379, 'meine': 380, 'meins': 381, 'meinst': 382, 'mich': 383, 'mir': 384, 'mit': 385, 'mutig': 386, 'möchte': 387, 'müde': 388, 'nach': 389, 'nachlassen': 390, 'nacht': 391, 'nahm': 392, 'nass': 393, 'nehmen': 394, 'nehmt': 395, 'nein': 396, 'nett': 397, 'neu': 398, 'neun': 399, 'nicht': 400, 'nichts': 401, 'nickte': 402, 'nieder': 403, 'niederlegen': 404, 'nimm': 405, 'nur': 406, 'nächste': 407, 'näher': 408, 'nüchtern': 409, 'ohnmacht': 410, 'ohnmächtig': 411, 'okay': 412, 'ordentlich': 413, 'ordnung': 414, 'osten': 415, 'pack': 416, 'pausierte': 417, 'pension': 418, 'perfekt': 419, 'pingelig': 420, 'plattenaufleger': 421, 'pleite': 422, 'potzdonner': 423, 'probier': 424, 'probiere': 425, 'probieren': 426, 'rannte': 427, 'rasiert': 428, 'rasierte': 429, 'rastete': 430, 'raus': 431, 'recht': 432, 'reden': 433, 'reg': 434, 'reich': 435, 'rein': 436, 'rennen': 437, 'rennt': 438, 'repariere': 439, 'reparieren': 440, 'rette': 441, 'retten': 442, 'rettet': 443, 'richtig': 444, 'richtung': 445, 'rot': 446, 'ruf': 447, 'rufe': 448, 'rufen': 449, 'ruft': 450, 'ruhe': 451, 'ruhig': 452, 'runden': 453, 'runter': 454, 'runzelte': 455, 'rutsch': 456, 'rutschte': 457, 'sag': 458, 'sagen': 459, 'sagenhaft': 460, 'sagt': 461, 'sagte': 462, 'sand': 463, 'sang': 464, 'satt': 465, 'sauer': 466, 'sause': 467, 'schau': 468, 'schauen': 469, 'schaut': 470, 'scheitern': 471, 'scher': 472, 'schieß': 473, 'schießen': 474, 'schlafen': 475, 'schlage': 476, 'schlagen': 477, 'schlagt': 478, 'schlecht': 479, 'schließ': 480, 'schließe': 481, 'schlimm': 482, 'schlug': 483, 'schmerzt': 484, 'schnapp': 485, 'schnarche': 486, 'schnell': 487, 'schon': 488, 'schrecklich': 489, 'schreib': 490, 'schreiben': 491, 'schreibt': 492, 'schwach': 493, 'schwamm': 494, 'schwimm': 495, 'schwimme': 496, 'schwirr': 497, 'schwur': 498, 'schön': 499, 'schönen': 500, 'schüchtern': 501, 'schüttete': 502, 'sehe': 503, 'sehen': 504, 'sei': 505, 'seid': 506, 'seien': 507, 'sein': 508, 'seinen': 509, 'seins': 510, 'seltsam': 511, 'sers': 512, 'setz': 513, 'setzen': 514, 'sexy': 515, 'sich': 516, 'sicher': 517, 'sie': 518, 'sieben': 519, 'sind': 520, 'singen': 521, 'ski': 522, 'so': 523, 'soweit': 524, 'spaß': 525, 'spion': 526, 'spionin': 527, 'sprach': 528, 'sprich': 529, 'spring': 530, 'spuckte': 531, 'spät': 532, 'später': 533, 'starb': 534, 'stecke': 535, 'steh': 536, 'stehe': 537, 'stehen': 538, 'stehenbleiben': 539, 'steht': 540, 'sterben': 541, 'still': 542, 'stimmen': 543, 'stirb': 544, 'stirn': 545, 'stopp': 546, 'stoppte': 547, 'stöhnte': 548, 'stück': 549, 'stürzte': 550, 'süß': 551, 'tapfer': 552, 'taub': 553, 'tee': 554, 'thomas': 555, 'tief': 556, 'toll': 557, 'tom': 558, 'tot': 559, 'traf': 560, 'traurig': 561, 'trete': 562, 'trocken': 563, 'troll': 564, 'träge': 565, 'tschüss': 566, 'tu': 567, 'tue': 568, 'tun': 569, 'tut': 570, 'töte': 571, 'töten': 572, 'tötet': 573, 'uhr': 574, 'um': 575, 'umarme': 576, 'umarmen': 577, 'umarmt': 578, 'und': 579, 'ungerecht': 580, 'unhöflich': 581, 'unmöglich': 582, 'uns': 583, 'unsers': 584, 'unten': 585, 'unverschämt': 586, 'verboten': 587, 'verbrannte': 588, 'verbrenne': 589, 'verbrennen': 590, 'verbrennt': 591, 'verdufte': 592, 'vergessen': 593, 'vergesst': 594, 'vergiss': 595, 'vergnügen': 596, 'vergrabe': 597, 'verirrt': 598, 'verkrümele': 599, 'verlasse': 600, 'verlassen': 601, 'verloren': 602, 'vermisse': 603, 'verpiss': 604, 'verrückt': 605, 'verschwinde': 606, 'verschwunden': 607, 'verspreche': 608, 'versprechs': 609, 'verstanden': 610, 'versteckt': 611, 'versteckte': 612, 'verstehe': 613, 'versuch': 614, 'versuchen': 615, 'versucht': 616, 'versuchte': 617, 'versuchten': 618, 'vertraue': 619, 'vertrauen': 620, 'vertraut': 621, 'verzieh': 622, 'verärgert': 623, 'viel': 624, 'vielleicht': 625, 'vier': 626, 'viertel': 627, 'voll': 628, 'vom': 629, 'von': 630, 'vorbei': 631, 'vorsicht': 632, 'vögel': 633, 'wach': 634, 'wachen': 635, 'wachte': 636, 'wahr': 637, 'war': 638, 'warm': 639, 'warne': 640, 'warnen': 641, 'warte': 642, 'warten': 643, 'wartet': 644, 'warum': 645, 'was': 646, 'wasch': 647, 'weg': 648, 'weh': 649, 'weigere': 650, 'weine': 651, 'weinen': 652, 'weint': 653, 'weinte': 654, 'weise': 655, 'weit': 656, 'weiter': 657, 'weiß': 658, 'wer': 659, 'werde': 660, 'werden': 661, 'wichtig': 662, 'wie': 663, 'wieder': 664, 'wiedersehen': 665, 'will': 666, 'willen': 667, 'willkommen': 668, 'winkte': 669, 'wir': 670, 'wird': 671, 'wirklich': 672, 'wohl': 673, 'wozu': 674, 'wunderbar': 675, 'wurde': 676, 'wusste': 677, 'wählerisch': 678, 'würde': 679, 'wütend': 680, 'zahlen': 681, 'zahlte': 682, 'zahn': 683, 'zeigs': 684, 'zieh': 685, 'zielen': 686, 'zisch': 687, 'zu': 688, 'zuhause': 689, 'zum': 690, 'zurecht': 691, 'zurück': 692, 'zusammen': 693, 'zutritt': 694, 'zwinkerte': 695, 'zäh': 696, 'ächzte': 697, 'änderte': 698, 'öffne': 699, 'öffnen': 700, 'öffnet': 701, 'über': 702, 'sos': 703}\n",
            "\n",
            "German Index-->Word : {1: 'ab', 2: 'abbrechen', 3: 'abend', 4: 'acker', 5: 'aha', 6: 'alle', 7: 'allein', 8: 'alt', 9: 'amüsiert', 10: 'an', 11: 'angriff', 12: 'anhalten', 13: 'anrufen', 14: 'antworten', 15: 'arbeit', 16: 'arm', 17: 'attacke', 18: 'auch', 19: 'auf', 20: 'aufgegeben', 21: 'aufgehört', 22: 'aufgelegt', 23: 'aufhören', 24: 'aufstehen', 25: 'aus', 26: 'ausgerutscht', 27: 'ausgeschieden', 28: 'ausgeschlossen', 29: 'aß', 30: 'bald', 31: 'beachte', 32: 'beachten', 33: 'bedürftig', 34: 'beeil', 35: 'beeilt', 36: 'begegnet', 37: 'behalt', 38: 'behalte', 39: 'behalten', 40: 'behaltet', 41: 'behebe', 42: 'beheben', 43: 'bei', 44: 'bekommen', 45: 'bellen', 46: 'benutze', 47: 'beobachte', 48: 'beobachten', 49: 'beobachtet', 50: 'bereit', 51: 'beruhige', 52: 'beruhigen', 53: 'bescheid', 54: 'beschleunigte', 55: 'beschäftigt', 56: 'bestürzt', 57: 'betete', 58: 'betrunken', 59: 'bett', 60: 'beweg', 61: 'bewegen', 62: 'bewegung', 63: 'bezahlt', 64: 'bin', 65: 'bis', 66: 'blau', 67: 'bleib', 68: 'bleiben', 69: 'bleibt', 70: 'blieb', 71: 'blind', 72: 'blinzelte', 73: 'brannte', 74: 'bring', 75: 'bringen', 76: 'bringt', 77: 'böse', 78: 'cool', 79: 'da', 80: 'dabei', 81: 'dageblieben', 82: 'danke', 83: 'daraus', 84: 'darf', 85: 'das', 86: 'davon', 87: 'deckung', 88: 'den', 89: 'der', 90: 'dich', 91: 'dick', 92: 'die', 93: 'dir', 94: 'dj', 95: 'doch', 96: 'dollar', 97: 'donnerwetter', 98: 'dran', 99: 'drauf', 100: 'dreh', 101: 'drück', 102: 'drücken', 103: 'drückt', 104: 'du', 105: 'dunkel', 106: 'dünn', 107: 'echt', 108: 'ein', 109: 'eine', 110: 'einen', 111: 'einfach', 112: 'eintritt', 113: 'einverstanden', 114: 'entkam', 115: 'entkommen', 116: 'entschuldigen', 117: 'entschuldigung', 118: 'entspann', 119: 'entspanne', 120: 'entspannt', 121: 'eos', 122: 'er', 123: 'ernst', 124: 'ernsthaft', 125: 'erste', 126: 'erster', 127: 'erzähl', 128: 'erzählen', 129: 'erzähls', 130: 'es', 131: 'essen', 132: 'etwas', 133: 'euch', 134: 'fahr', 135: 'fahren', 136: 'fahrt', 137: 'fair', 138: 'fallen', 139: 'falte', 140: 'fang', 141: 'fange', 142: 'fangen', 143: 'fangt', 144: 'fantastisch', 145: 'fass', 146: 'fassen', 147: 'fasst', 148: 'faul', 149: 'fehlt', 150: 'feier', 151: 'fernseher', 152: 'fertig', 153: 'fest', 154: 'fett', 155: 'feuer', 156: 'fiel', 157: 'finde', 158: 'finden', 159: 'findet', 160: 'fliege', 161: 'fliegen', 162: 'fluchte', 163: 'flüchtete', 164: 'folge', 165: 'fort', 166: 'frag', 167: 'frage', 168: 'fragen', 169: 'fragt', 170: 'frei', 171: 'froh', 172: 'fror', 173: 'früh', 174: 'fuhr', 175: 'funktioniert', 176: 'fuß', 177: 'füge', 178: 'fühl', 179: 'fünf', 180: 'fünfundvierzig', 181: 'für', 182: 'fürchte', 183: 'gab', 184: 'ganz', 185: 'gar', 186: 'gebe', 187: 'gefahren', 188: 'gefallen', 189: 'geflucht', 190: 'geflüchtet', 191: 'gefroren', 192: 'gefällt', 193: 'gegangen', 194: 'gegessen', 195: 'gegähnt', 196: 'geh', 197: 'gehe', 198: 'geheilt', 199: 'gehen', 200: 'geholfen', 201: 'gehorchen', 202: 'geht', 203: 'gehts', 204: 'gehustet', 205: 'gehöre', 206: 'gehörlos', 207: 'gehört', 208: 'geklappt', 209: 'gekommen', 210: 'gekündigt', 211: 'gelangweilt', 212: 'gelassen', 213: 'gelogen', 214: 'gemacht', 215: 'gemein', 216: 'gemeint', 217: 'genommen', 218: 'gerannt', 219: 'gerecht', 220: 'geschafft', 221: 'geschlagen', 222: 'geschneit', 223: 'geschummelt', 224: 'geschwommen', 225: 'geschworen', 226: 'gesehen', 227: 'gesicht', 228: 'gesprungen', 229: 'gespuckt', 230: 'gestorben', 231: 'gestürzt', 232: 'gesundheit', 233: 'getroffen', 234: 'gewartet', 235: 'geweint', 236: 'gewinne', 237: 'gewinnen', 238: 'gewonnen', 239: 'gewunken', 240: 'gewusst', 241: 'gezahlt', 242: 'geöffnet', 243: 'ging', 244: 'glatze', 245: 'glatzköpfig', 246: 'glücklich', 247: 'gott', 248: 'greif', 249: 'groß', 250: 'grüß', 251: 'guck', 252: 'gut', 253: 'gute', 254: 'gähnte', 255: 'hab', 256: 'habe', 257: 'haben', 258: 'habs', 259: 'habt', 260: 'halb', 261: 'half', 262: 'hallo', 263: 'halt', 264: 'halte', 265: 'halten', 266: 'haltet', 267: 'handschellen', 268: 'hart', 269: 'hast', 270: 'hat', 271: 'hatte', 272: 'hau', 273: 'hause', 274: 'heim', 275: 'heirate', 276: 'heiß', 277: 'helfe', 278: 'helfen', 279: 'helft', 280: 'her', 281: 'herein', 282: 'hervorragend', 283: 'hielt', 284: 'hier', 285: 'hierher', 286: 'hilf', 287: 'hilfe', 288: 'hilft', 289: 'hin', 290: 'hingefallen', 291: 'hinlegen', 292: 'hinten', 293: 'hinter', 294: 'hoch', 295: 'hoffe', 296: 'hol', 297: 'hols', 298: 'hunde', 299: 'hustete', 300: 'hände', 301: 'hässlich', 302: 'hör', 303: 'hört', 304: 'hörte', 305: 'hülf', 306: 'ich', 307: 'ihm', 308: 'ihn', 309: 'ihr', 310: 'im', 311: 'in', 312: 'ins', 313: 'iss', 314: 'ist', 315: 'ja', 316: 'jahre', 317: 'jetzt', 318: 'kalt', 319: 'kam', 320: 'kann', 321: 'kannst', 322: 'kapiert', 323: 'kasse', 324: 'kein', 325: 'keine', 326: 'keinster', 327: 'kicherte', 328: 'klappt', 329: 'klar', 330: 'klatschte', 331: 'knapp', 332: 'knicken', 333: 'kochen', 334: 'komm', 335: 'komme', 336: 'kommen', 337: 'kommt', 338: 'kopf', 339: 'krank', 340: 'kurz', 341: 'können', 342: 'kühl', 343: 'küsse', 344: 'küssen', 345: 'küsst', 346: 'lachte', 347: 'lage', 348: 'langweilig', 349: 'lass', 350: 'lassen', 351: 'lasst', 352: 'lauf', 353: 'laufen', 354: 'lauter', 355: 'leb', 356: 'leben', 357: 'leg', 358: 'legen', 359: 'legt', 360: 'leid', 361: 'leine', 362: 'liebe', 363: 'lief', 364: 'lieg', 365: 'locker', 366: 'log', 367: 'losgehen', 368: 'lächeln', 369: 'lächelte', 370: 'läufst', 371: 'läuft', 372: 'lüge', 373: 'lügt', 374: 'mach', 375: 'mache', 376: 'macht', 377: 'mal', 378: 'mann', 379: 'maul', 380: 'meine', 381: 'meins', 382: 'meinst', 383: 'mich', 384: 'mir', 385: 'mit', 386: 'mutig', 387: 'möchte', 388: 'müde', 389: 'nach', 390: 'nachlassen', 391: 'nacht', 392: 'nahm', 393: 'nass', 394: 'nehmen', 395: 'nehmt', 396: 'nein', 397: 'nett', 398: 'neu', 399: 'neun', 400: 'nicht', 401: 'nichts', 402: 'nickte', 403: 'nieder', 404: 'niederlegen', 405: 'nimm', 406: 'nur', 407: 'nächste', 408: 'näher', 409: 'nüchtern', 410: 'ohnmacht', 411: 'ohnmächtig', 412: 'okay', 413: 'ordentlich', 414: 'ordnung', 415: 'osten', 416: 'pack', 417: 'pausierte', 418: 'pension', 419: 'perfekt', 420: 'pingelig', 421: 'plattenaufleger', 422: 'pleite', 423: 'potzdonner', 424: 'probier', 425: 'probiere', 426: 'probieren', 427: 'rannte', 428: 'rasiert', 429: 'rasierte', 430: 'rastete', 431: 'raus', 432: 'recht', 433: 'reden', 434: 'reg', 435: 'reich', 436: 'rein', 437: 'rennen', 438: 'rennt', 439: 'repariere', 440: 'reparieren', 441: 'rette', 442: 'retten', 443: 'rettet', 444: 'richtig', 445: 'richtung', 446: 'rot', 447: 'ruf', 448: 'rufe', 449: 'rufen', 450: 'ruft', 451: 'ruhe', 452: 'ruhig', 453: 'runden', 454: 'runter', 455: 'runzelte', 456: 'rutsch', 457: 'rutschte', 458: 'sag', 459: 'sagen', 460: 'sagenhaft', 461: 'sagt', 462: 'sagte', 463: 'sand', 464: 'sang', 465: 'satt', 466: 'sauer', 467: 'sause', 468: 'schau', 469: 'schauen', 470: 'schaut', 471: 'scheitern', 472: 'scher', 473: 'schieß', 474: 'schießen', 475: 'schlafen', 476: 'schlage', 477: 'schlagen', 478: 'schlagt', 479: 'schlecht', 480: 'schließ', 481: 'schließe', 482: 'schlimm', 483: 'schlug', 484: 'schmerzt', 485: 'schnapp', 486: 'schnarche', 487: 'schnell', 488: 'schon', 489: 'schrecklich', 490: 'schreib', 491: 'schreiben', 492: 'schreibt', 493: 'schwach', 494: 'schwamm', 495: 'schwimm', 496: 'schwimme', 497: 'schwirr', 498: 'schwur', 499: 'schön', 500: 'schönen', 501: 'schüchtern', 502: 'schüttete', 503: 'sehe', 504: 'sehen', 505: 'sei', 506: 'seid', 507: 'seien', 508: 'sein', 509: 'seinen', 510: 'seins', 511: 'seltsam', 512: 'sers', 513: 'setz', 514: 'setzen', 515: 'sexy', 516: 'sich', 517: 'sicher', 518: 'sie', 519: 'sieben', 520: 'sind', 521: 'singen', 522: 'ski', 523: 'so', 524: 'soweit', 525: 'spaß', 526: 'spion', 527: 'spionin', 528: 'sprach', 529: 'sprich', 530: 'spring', 531: 'spuckte', 532: 'spät', 533: 'später', 534: 'starb', 535: 'stecke', 536: 'steh', 537: 'stehe', 538: 'stehen', 539: 'stehenbleiben', 540: 'steht', 541: 'sterben', 542: 'still', 543: 'stimmen', 544: 'stirb', 545: 'stirn', 546: 'stopp', 547: 'stoppte', 548: 'stöhnte', 549: 'stück', 550: 'stürzte', 551: 'süß', 552: 'tapfer', 553: 'taub', 554: 'tee', 555: 'thomas', 556: 'tief', 557: 'toll', 558: 'tom', 559: 'tot', 560: 'traf', 561: 'traurig', 562: 'trete', 563: 'trocken', 564: 'troll', 565: 'träge', 566: 'tschüss', 567: 'tu', 568: 'tue', 569: 'tun', 570: 'tut', 571: 'töte', 572: 'töten', 573: 'tötet', 574: 'uhr', 575: 'um', 576: 'umarme', 577: 'umarmen', 578: 'umarmt', 579: 'und', 580: 'ungerecht', 581: 'unhöflich', 582: 'unmöglich', 583: 'uns', 584: 'unsers', 585: 'unten', 586: 'unverschämt', 587: 'verboten', 588: 'verbrannte', 589: 'verbrenne', 590: 'verbrennen', 591: 'verbrennt', 592: 'verdufte', 593: 'vergessen', 594: 'vergesst', 595: 'vergiss', 596: 'vergnügen', 597: 'vergrabe', 598: 'verirrt', 599: 'verkrümele', 600: 'verlasse', 601: 'verlassen', 602: 'verloren', 603: 'vermisse', 604: 'verpiss', 605: 'verrückt', 606: 'verschwinde', 607: 'verschwunden', 608: 'verspreche', 609: 'versprechs', 610: 'verstanden', 611: 'versteckt', 612: 'versteckte', 613: 'verstehe', 614: 'versuch', 615: 'versuchen', 616: 'versucht', 617: 'versuchte', 618: 'versuchten', 619: 'vertraue', 620: 'vertrauen', 621: 'vertraut', 622: 'verzieh', 623: 'verärgert', 624: 'viel', 625: 'vielleicht', 626: 'vier', 627: 'viertel', 628: 'voll', 629: 'vom', 630: 'von', 631: 'vorbei', 632: 'vorsicht', 633: 'vögel', 634: 'wach', 635: 'wachen', 636: 'wachte', 637: 'wahr', 638: 'war', 639: 'warm', 640: 'warne', 641: 'warnen', 642: 'warte', 643: 'warten', 644: 'wartet', 645: 'warum', 646: 'was', 647: 'wasch', 648: 'weg', 649: 'weh', 650: 'weigere', 651: 'weine', 652: 'weinen', 653: 'weint', 654: 'weinte', 655: 'weise', 656: 'weit', 657: 'weiter', 658: 'weiß', 659: 'wer', 660: 'werde', 661: 'werden', 662: 'wichtig', 663: 'wie', 664: 'wieder', 665: 'wiedersehen', 666: 'will', 667: 'willen', 668: 'willkommen', 669: 'winkte', 670: 'wir', 671: 'wird', 672: 'wirklich', 673: 'wohl', 674: 'wozu', 675: 'wunderbar', 676: 'wurde', 677: 'wusste', 678: 'wählerisch', 679: 'würde', 680: 'wütend', 681: 'zahlen', 682: 'zahlte', 683: 'zahn', 684: 'zeigs', 685: 'zieh', 686: 'zielen', 687: 'zisch', 688: 'zu', 689: 'zuhause', 690: 'zum', 691: 'zurecht', 692: 'zurück', 693: 'zusammen', 694: 'zutritt', 695: 'zwinkerte', 696: 'zäh', 697: 'ächzte', 698: 'änderte', 699: 'öffne', 700: 'öffnen', 701: 'öffnet', 702: 'über', 703: 'sos'}\n",
            "\n",
            "Eng Dictionary Size:  380\n",
            "German Dictionary Size: 703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAOrCyW4zWcl"
      },
      "source": [
        "###Finding length of longest sentence in the train dataset for both languages\n",
        "To be used as no. of time steps for the encoder-decoder model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiyQbnPPhW1J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6056bf9-36d8-456b-ab0a-88b20e21eaf8"
      },
      "source": [
        "#Finding maximum length of sentence in English and German sentences\n",
        "def max_length(sentences):\n",
        "  max_length = 1\n",
        "  for i in sentences:\n",
        "    for j in i:\n",
        "      words = j.split()\n",
        "      length = len(words)\n",
        "\n",
        "    if length >= max_length:\n",
        "      max_length = length\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  return(max_length)\n",
        "\n",
        "max_eng_length = max_length(clean_input_sentences)\n",
        "max_german_length = max_length(clean_output_sentences)\n",
        "\n",
        "print(\"Length of longest English sentence :\", max_eng_length)\n",
        "print(\"Length of longest German sentence :\", max_german_length)\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of longest English sentence : 3\n",
            "Length of longest German sentence : 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lb1Re0SSzVLv"
      },
      "source": [
        "###Text-Preprocessing\n",
        "\n",
        "*   Tokenization\n",
        "*   Padding\n",
        " - Pre-Padding for Encoder Model\n",
        " *   Post-Padding for Decoder Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzLu7RfzlrXt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f64bcf25-d835-4e71-9df5-1c1d402f8687"
      },
      "source": [
        "#Tokenization and Padding\n",
        "\n",
        "def tokenization(sentences,para):\n",
        "  final_token = []\n",
        "  for i in sentences:\n",
        "    for j in i:\n",
        "      words = j.split()\n",
        "      token = []\n",
        "      for x in words:\n",
        "        if para ==0:\n",
        "          index = vocab_eng[x]\n",
        "        else:\n",
        "          index = vocab_german[x]\n",
        "        token.append(index)\n",
        "    final_token.append(token)\n",
        "  \n",
        "  return(final_token)\n",
        "\n",
        "token_eng = tokenization(clean_input_sentences,0)\n",
        "token_german_inp = tokenization(clean_output_sentences_inputs,1)\n",
        "token_german_out = tokenization(clean_output_sentences,1)\n",
        "\n",
        "print(\"After tokenization English Sentences = \", token_eng[959])\n",
        "print(\"After tokenization German Input Sentences = \", token_german_inp[959])\n",
        "print(\"After tokenization German Output = \", token_german_out[959])\n",
        "\n",
        "\n",
        "#Pre padding for Input sentences and Post padding for Output sentences\n",
        "def padding(token,para):\n",
        "  thislist = []\n",
        "  if para == 0:\n",
        "    length = max_eng_length\n",
        "    for x in token:\n",
        "      pad_arr = np.pad(x , (length-len(x),0) , 'constant', constant_values=0)\n",
        "      arrlist = pad_arr.tolist()\n",
        "      thislist.append(arrlist)\n",
        "      \n",
        "  else:\n",
        "    length = max_german_length\n",
        "    for x in token:\n",
        "      pad_arr = np.pad(x , (0,length-len(x)) , 'constant', constant_values=0)   \n",
        "      arrlist = pad_arr.tolist()\n",
        "      thislist.append(arrlist)\n",
        "      \n",
        "  return(thislist)\n",
        "\n",
        "\n",
        "padded_eng = padding(token_eng,0)\n",
        "padded_german_inp = padding(token_german_inp,1)\n",
        "padded_german_out = padding(token_german_out,1)\n",
        "\n",
        "print(\"\\nAfter Padding English Sentences = \" , padded_eng[959])\n",
        "print(\"After Padding Gemrman Input Sentences = \" , padded_german_inp[959])\n",
        "print(\"After Padding Gemrman Output Sentences = \" , padded_german_out[959])\n",
        "    \n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After tokenization English Sentences =  [176, 319]\n",
            "After tokenization German Input Sentences =  [703, 37, 518]\n",
            "After tokenization German Output =  [37, 518, 121]\n",
            "\n",
            "After Padding English Sentences =  [0, 176, 319]\n",
            "After Padding Gemrman Input Sentences =  [703, 37, 518, 0, 0, 0, 0]\n",
            "After Padding Gemrman Output Sentences =  [37, 518, 121, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LckoLL811Z9w"
      },
      "source": [
        "###Word Embedding\n",
        "Creating word embedding matrix for english input sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgkE8cTD6Gg6"
      },
      "source": [
        "#Word Embedding for Eng lang input layer\n",
        "\n",
        "embeddings_dictionary = {}\n",
        "\n",
        "glove_file = open(r'/content/glove.6B.50d.txt', encoding=\"utf8\")\n",
        "\n",
        "#Reading the required words from the emedding file and storing the required in code for further initialisation\n",
        "for line in glove_file:\n",
        "    records = line.split()\n",
        "    word = records[0]\n",
        "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
        "    embeddings_dictionary[word] = vector_dimensions\n",
        "glove_file.close()\n",
        "\n",
        "\"\"\"Creating a embedding matrix with row number is the integer value of the word and the column will represent corresponding \n",
        "embedding vector for the word\n",
        "\"\"\"\n",
        "embedding_matrix = np.zeros((vocab_eng_size+1, 50))   #Index 0 will have no values \n",
        "for word, index in vocab_eng.items():\n",
        "    embedding_vector = embeddings_dictionary.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector\n",
        "\n",
        "        \n",
        "\"\"\"\n",
        "Creating Decoder Input and Output layer (one hot) for german language with row number is the integer value of the word and the \n",
        "column will represent corresponding one hot vector for the word with 1 value at the index position \n",
        "corresponing to the integer value of the word\n",
        "\"\"\"\n",
        "german_matrix = np.zeros((vocab_german_size+1, vocab_german_size))\n",
        "for word, index in vocab_german.items():\n",
        "     german_matrix[index,index-1] = 1\n",
        "     "
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtgsfokDp_y_"
      },
      "source": [
        "# Labeling each important variable for reference\n",
        "\n",
        "input_examples #No. of English Senteces\n",
        "output_examples #No.of German Sentences = above\n",
        "vocab_german_size #No. of German words\n",
        "max_eng_length #Encoder Time Steps\n",
        "max_german_length #Decoder Time Steps\n",
        "embedding_matrix #Input Embedding Matrix\n",
        "german_matrix #Output German Matrix\n",
        "\n",
        "padded_eng = np.array(padded_eng) #X_enc_train\n",
        "padded_german_inp = np.array(padded_german_inp) #X_dec_train\n",
        "padded_german_out = np.array(padded_german_out) #Ytrain"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUPtBEI424g7"
      },
      "source": [
        "###Forming Tensors for the Model\n",
        "\n",
        "*   Input 3D Tensor of English sentences for Encoder Model\n",
        "*   Input 3D Tensor of German sentencess for Decoder Model\n",
        "*   Output 3D Tensor of Gemrman sentences for Decoder Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8xiDSiOrrAW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b80eba9-b560-49a8-b3b6-0f40d25aab6f"
      },
      "source": [
        "#Forming Input 3D Tensors\n",
        "\n",
        "#Converting the input sequence into embedding and reshaping a 3D tensor of shape(Embedding dim, Number of sentences, Timestep)\n",
        "Xenc_train=np.zeros((50, input_examples , max_eng_length))\n",
        "for m in range(input_examples):\n",
        "    for t in range(max_eng_length):\n",
        "        Xenc_train[:,m,t]=embedding_matrix[padded_eng[m,t]]\n",
        "\n",
        "#Converting the output sequence into one-hot and reshaping a 3D tensor of shape(ger_vocab_size, Number of sentences, Timestep)\n",
        "Xdec_train = np.zeros((vocab_german_size, input_examples , max_german_length))\n",
        "for m in range(input_examples): \n",
        "  for t in range(max_german_length):\n",
        "    Xdec_train[:,m,t] = german_matrix[padded_german_inp[m,t]]\n",
        "\n",
        "Ydec_train = np.zeros((vocab_german_size, input_examples , max_german_length))\n",
        "for m in range(input_examples): \n",
        "  for t in range(max_german_length):\n",
        "    Ydec_train[:,m,t] = german_matrix[padded_german_out[m,t]]\n",
        "\n",
        "print(Xenc_train[:,959,:])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.          0.14816999  0.64642   ]\n",
            " [ 0.         -0.22174001 -0.55599999]\n",
            " [ 0.          0.60220999  0.47038001]\n",
            " [ 0.         -0.79488999 -0.82073998]\n",
            " [ 0.          0.33458     0.79512   ]\n",
            " [ 0.         -0.40401     0.28771001]\n",
            " [ 0.         -0.62263    -0.56426001]\n",
            " [ 0.          0.2534      0.1463    ]\n",
            " [ 0.          0.054533   -0.52420998]\n",
            " [ 0.         -0.032518    0.021607  ]\n",
            " [ 0.         -0.045699   -0.11266   ]\n",
            " [ 0.          0.44084999  0.31986001]\n",
            " [ 0.         -0.4429     -0.057542  ]\n",
            " [ 0.          0.28121999 -0.23375   ]\n",
            " [ 0.          0.36605999  0.63703001]\n",
            " [ 0.          0.78263003  0.3382    ]\n",
            " [ 0.          0.50840998  0.46489999]\n",
            " [ 0.         -0.36081001 -0.42398   ]\n",
            " [ 0.          0.38600001  0.091868  ]\n",
            " [ 0.         -1.02349997 -0.8804    ]\n",
            " [ 0.          0.10762     0.22077   ]\n",
            " [ 0.          0.11578     0.71270001]\n",
            " [ 0.          0.25240001  0.98196   ]\n",
            " [ 0.         -0.025313   -0.033819  ]\n",
            " [ 0.          0.026976    0.31553999]\n",
            " [ 0.         -1.76540005 -1.80009997]\n",
            " [ 0.          0.21529    -0.26003   ]\n",
            " [ 0.          0.23379999 -0.37762001]\n",
            " [ 0.          1.0036      0.85633999]\n",
            " [ 0.         -0.42980999 -1.33930004]\n",
            " [ 0.          3.39280009  3.6408    ]\n",
            " [ 0.          1.06760001  0.87273002]\n",
            " [ 0.         -0.42449    -0.79229999]\n",
            " [ 0.         -0.082618   -0.51267999]\n",
            " [ 0.         -0.36884001  0.14415   ]\n",
            " [ 0.          0.17497     0.55439001]\n",
            " [ 0.         -0.28571999  0.1006    ]\n",
            " [ 0.          0.17851999  0.13233   ]\n",
            " [ 0.          0.19159999 -0.071904  ]\n",
            " [ 0.         -0.67692    -0.16399001]\n",
            " [ 0.         -0.18997    -0.089989  ]\n",
            " [ 0.         -0.10071    -0.031932  ]\n",
            " [ 0.          0.22753     0.61390001]\n",
            " [ 0.          0.50452     0.40419   ]\n",
            " [ 0.          0.2432      0.22686   ]\n",
            " [ 0.         -0.03997    -0.18974   ]\n",
            " [ 0.         -0.089887   -0.55403   ]\n",
            " [ 0.         -0.19582    -0.35831001]\n",
            " [ 0.         -0.096731   -0.10995   ]\n",
            " [ 0.         -0.055916   -0.447     ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WchkRLf4H18"
      },
      "source": [
        "###Encoder-Decoder Model\n",
        "Following are the function being implemented :\n",
        "*   Parameters Initialization\n",
        "*   Encoder Forward Propogation\n",
        "*   Decoder Forward Propogation\n",
        "*   Loss - Cross Entropy Loss\n",
        "*   Decoder Backward Propogation\n",
        "*   Encoder Backward Propogation\n",
        "*   Gradient Descent\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-f0gtjhrrNH"
      },
      "source": [
        "class LSTM:\n",
        "   \n",
        "\n",
        "    def __init__(self, embed_enc, onehot_dec, eng_vocab, ger_vocab, epoch, lr, hid_dim, embed_size, timesteps_enc , timesteps_dec):\n",
        "        self.embed_enc=embed_enc  #Embedded input\n",
        "        self.onehot_dec=onehot_dec #Decoder OneHot Input\n",
        "        self.epoch=epoch  #number of epoch\n",
        "        self.lr=lr   #learning rate\n",
        "\n",
        "        self.embed_size=embed_size   #(n_x)_encoder\n",
        "        self.ger_vocab=ger_vocab     #(n_x)_decoder and n_y\n",
        "        self.eng_vocab=eng_vocab\n",
        "        self.length_enc = timesteps_enc   #Encoder Timesteps\n",
        "        self.length_dec = timesteps_dec   #Decoder Timesteps\n",
        "        self.hidden = hid_dim        #(n_a)\n",
        "        \n",
        "        #Parameter Initialization\n",
        "\n",
        "        np.random.seed(1)\n",
        "        #Encoder Paramters\n",
        "        self.wf_enc = np.random.randn(hid_dim , hid_dim + embed_size)*0.01\n",
        "        self.wi_enc = np.random.randn(hid_dim , hid_dim + embed_size)*0.01\n",
        "        self.wo_enc = np.random.randn(hid_dim , hid_dim + embed_size)*0.01\n",
        "        self.wc_enc = np.random.randn(hid_dim , hid_dim + embed_size)*0.01\n",
        "\n",
        "        self.bf_enc = np.zeros((hid_dim , 1))\n",
        "        self.bi_enc = np.zeros((hid_dim , 1))\n",
        "        self.bo_enc = np.zeros((hid_dim , 1))\n",
        "        self.bc_enc = np.zeros((hid_dim , 1))\n",
        "\n",
        "        #Decoder Paramters\n",
        "        self.wf_dec = np.random.randn(hid_dim , hid_dim + ger_vocab)*0.01\n",
        "        self.wi_dec = np.random.randn(hid_dim , hid_dim + ger_vocab)*0.01\n",
        "        self.wo_dec = np.random.randn(hid_dim , hid_dim + ger_vocab)*0.01\n",
        "        self.wc_dec = np.random.randn(hid_dim , hid_dim + ger_vocab)*0.01\n",
        "        self.bf_dec = np.zeros((hid_dim , 1))\n",
        "        self.bi_dec = np.zeros((hid_dim , 1))\n",
        "        self.bo_dec = np.zeros((hid_dim , 1))\n",
        "        self.bc_dec = np.zeros((hid_dim , 1))\n",
        "\n",
        "        self.wy_dec = np.random.randn(ger_vocab , hid_dim )*0.01\n",
        "        self.by_dec = np.zeros((ger_vocab , 1))\n",
        "\n",
        "        #Encoder Gradients \n",
        "        self.dwf_enc = np.zeros_like(self.wf_enc)\n",
        "        self.dwi_enc = np.zeros_like(self.wi_enc)\n",
        "        self.dwo_enc = np.zeros_like(self.wo_enc)\n",
        "        self.dwc_enc = np.zeros_like(self.wc_enc)\n",
        "        \n",
        "        self.dbf_enc = np.zeros_like(self.bf_enc)\n",
        "        self.dbi_enc = np.zeros_like(self.bi_enc)\n",
        "        self.dbo_enc = np.zeros_like(self.bo_enc)\n",
        "        self.dbc_enc = np.zeros_like(self.bc_enc)\n",
        "     \n",
        "        #Decoder Gradients\n",
        "        self.dwf_dec = np.zeros_like(self.wf_dec)\n",
        "        self.dwi_dec = np.zeros_like(self.wi_dec)\n",
        "        self.dwo_dec = np.zeros_like(self.wo_dec)\n",
        "        self.dwc_dec = np.zeros_like(self.wc_dec)\n",
        "\n",
        "        self.dbf_dec = np.zeros_like(self.bf_dec)\n",
        "        self.dbi_dec = np.zeros_like(self.bi_dec)\n",
        "        self.dbo_dec = np.zeros_like(self.bo_dec)\n",
        "        self.dbc_dec = np.zeros_like(self.bc_dec)\n",
        "\n",
        "        self.dwy_dec = np.zeros_like(self.wy_dec)\n",
        "        self.dby_dec = np.zeros_like(self.by_dec)\n",
        "\n",
        "        self.daa_dec=np.zeros((hid_dim, embed_enc.shape[1]))   ##\n",
        "        self.daa_enc=np.zeros_like(self.daa_dec)              ##\n",
        "        self.dcc_dec=np.zeros((hid_dim, embed_enc.shape[1]))   ##\n",
        "        self.dcc_enc=np.zeros_like(self.dcc_dec)               ##\n",
        "    \n",
        "\n",
        "    def encoder_forward(self):\n",
        "\n",
        "      def sigmoid(x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "      #Forward prop in Encoder\n",
        "      enc_hidden={}\n",
        "      enc_memory = {}\n",
        "      enc_ft = {}\n",
        "      enc_it = {}\n",
        "      enc_ot = {}\n",
        "      enc_cct = {}\n",
        "\n",
        "      a_next = np.zeros((self.wf_enc.shape[0], self.embed_enc.shape[1]))\n",
        "      c_next = np.zeros((self.wf_enc.shape[0], self.embed_enc.shape[1]))\n",
        "      enc_hidden[-1] = a_next\n",
        "      enc_memory[-1] = c_next\n",
        "\n",
        "      for t in range(self.length_enc):\n",
        "\n",
        "        xt = self.embed_enc[:,:,t]\n",
        "        concat = np.concatenate((a_next , xt))\n",
        "        ft = sigmoid( self.wf_enc @ concat + self.bf_enc )\n",
        "        it = sigmoid( self.wi_enc @ concat + self.bi_enc )\n",
        "        ot = sigmoid( self.wo_enc @ concat + self.bo_enc)\n",
        "        cct = np.tanh(self.wc_enc @ concat + self.bc_enc )\n",
        "\n",
        "        enc_ft[t] = ft\n",
        "        enc_it[t] = it \n",
        "        enc_ot[t] = ot\n",
        "        enc_cct[t] = cct\n",
        "\n",
        "        c_next = ((ft*c_next) + (it*cct))\n",
        "        a_next = ot*(np.tanh(c_next))\n",
        "\n",
        "        enc_hidden[t] = a_next\n",
        "        enc_memory[t] = c_next\n",
        "\n",
        "\n",
        "      return enc_hidden , enc_memory ,  enc_ft, enc_it, enc_ot, enc_cct\n",
        "\n",
        "    def decoder_forward(self, enc_hidden, enc_memory):\n",
        "\n",
        "        def sigmoid(x):\n",
        "          return 1 / (1 + np.exp(-x))\n",
        "\n",
        "        def softmax(x):\n",
        "          e_x = np.exp(x - np.max(x))\n",
        "          return e_x / e_x.sum(axis=0)\n",
        "\n",
        "\n",
        "        #Forward prop in Decoder\n",
        "        #dec_output = []\n",
        "        #dec_input = []\n",
        "        dec_hidden = {}\n",
        "        dec_memory = {}\n",
        "        dec_output = {}\n",
        "        dec_ft = {}\n",
        "        dec_it = {}\n",
        "        dec_ot = {}\n",
        "        dec_cct = {}\n",
        "        \n",
        "        a_next = enc_hidden\n",
        "        c_next = enc_memory\n",
        "        dec_hidden[-1] = enc_hidden\n",
        "        dec_memory[-1] = enc_memory\n",
        "\n",
        "        for t in range(self.length_dec):\n",
        "\n",
        "          xt = self.onehot_dec[:,:,t]\n",
        "          concat = np.concatenate((a_next , xt))\n",
        "          ft = sigmoid(self.wf_dec @ concat + self.bf_dec )\n",
        "          it = sigmoid(self.wi_dec @ concat + self.bi_dec )\n",
        "          ot = sigmoid(self.wo_dec @ concat + self.bo_dec )\n",
        "          cct = np.tanh(self.wc_dec @ concat + self.bc_dec )\n",
        "          dec_ft[t] = ft\n",
        "          dec_it[t] = it \n",
        "          dec_ot[t] = ot\n",
        "          dec_cct[t] = cct\n",
        "\n",
        "          c_next = ((ft*c_next) + (it*cct))\n",
        "          a_next = ot*(np.tanh(c_next))\n",
        "\n",
        "          y_pred = softmax( self.wy_dec @ a_next + self.by_dec )\n",
        "\n",
        "          dec_hidden[t] = a_next\n",
        "          dec_memory[t] = c_next\n",
        "          dec_output[t] = y_pred\n",
        "\n",
        "\n",
        "        return dec_hidden , dec_memory , dec_output, dec_ft, dec_it, dec_ot, dec_cct\n",
        "\n",
        "\n",
        "    def loss(self, pred, orig):                                                   \n",
        "        #compute loss\n",
        "        loss=0\n",
        "        for t in range(orig.shape[2]):\n",
        "            yt_pred = pred[t]              #Since pred is dictionary with each key being a timestep have value of y pred - array(n_y , m)\n",
        "            for m in range(orig.shape[1]):\n",
        "              loss+= -np.sum((orig[:,m,t]*np.log(yt_pred[:,m])))\n",
        "\n",
        "\n",
        "        return loss/orig.shape[1]\n",
        "\n",
        "    def decoder_backward(self, dec_out, dec_hidden, dec_memory, orig, dec_ft, dec_it, dec_ot, dec_cct):\n",
        "        #Backpropagation through time\n",
        "\n",
        "        for t in reversed(range(self.length_dec)): \n",
        "            ft = dec_ft[t]\n",
        "            it = dec_it[t] \n",
        "            ot = dec_ot[t]\n",
        "            cct = dec_cct[t]\n",
        "            xt = self.onehot_dec[:,:,t]\n",
        "            c_next = dec_memory[t]\n",
        "            c_prev = dec_memory[t-1]\n",
        "            a_next = dec_hidden[t]\n",
        "            a_prev = dec_hidden[t-1]\n",
        "            n_a = self.hidden\n",
        "   \n",
        "            dy = np.copy(dec_out[t])                          #Backpropogation through Softmax\n",
        "            Y = orig[:,:,t]\n",
        "            for m in range(orig.shape[1]):\n",
        "                dy[:,m] -= Y[:,m]\n",
        "\n",
        "            self.dwy_dec += np.dot(dy, dec_hidden[t].T)       #Backpropogation through DenseLayer\n",
        "            self.dby_dec += np.sum(dy, axis=1, keepdims=True)\n",
        "\n",
        "            da = np.dot(self.wy_dec.T, dy) + self.daa_dec     #Backprop addidtion from DenseLayer and Reccurentlayer\n",
        "            dot = da*np.tanh(c_next)*ot*(1 - ot)\n",
        "            dcct = (self.dcc_dec*it + da*ot*(1- (np.tanh(c_next)**2))*it)*(1-cct**2)\n",
        "            dit = (self.dcc_dec*cct + da*ot*(1- (np.tanh(c_next)**2))*cct)*it*(1 - it)\n",
        "            dft = (self.dcc_dec*c_prev + da*ot*(1- (np.tanh(c_next)**2))*c_prev)*ft*(1 - ft)\n",
        "\n",
        "            self.dwf_dec += dft@(np.concatenate((a_prev , xt))).T\n",
        "            self.dwi_dec += dit@(np.concatenate((a_prev , xt))).T\n",
        "            self.dwc_dec += dcct@(np.concatenate((a_prev , xt))).T\n",
        "            self.dwo_dec += dot@(np.concatenate((a_prev , xt))).T\n",
        "            self.dbf_dec += np.sum(dft , axis = 1 , keepdims = True)\n",
        "            self.dbi_dec += np.sum(dit , axis = 1 , keepdims = True)\n",
        "            self.dbc_dec += np.sum(dcct , axis = 1 , keepdims = True)\n",
        "            self.dbo_dec += np.sum(dot , axis = 1 , keepdims = True)\n",
        "\n",
        "            self.daa_dec = self.wf_dec[:,:n_a].T@dft + self.wi_dec[:,:n_a].T@dit + self.wc_dec[:,:n_a].T@dcct + self.wo_dec[:,:n_a].T@dot\n",
        "            self.dcc_dec = self.dcc_dec*ft + da*ot*(1-(np.tanh(c_next)**2))*ft\n",
        "        pass\n",
        "\n",
        "    def encoder_backward(self, enc_hidden, enc_memory, enc_ft, enc_it, enc_ot, enc_cct):\n",
        "        #Backpropagation through time\n",
        "        self.daa_enc = self.daa_dec  #Passing Decoder to Encoder (Hidden State)\n",
        "        self.dcc_enc = self.dcc_dec  #Passing Decoder to Encoder (Cell Memory)\n",
        "        for t in reversed(range(self.length_enc)): \n",
        "            ft = enc_ft[t]\n",
        "            it = enc_it[t] \n",
        "            ot = enc_ot[t]\n",
        "            cct = enc_cct[t]\n",
        "            xt = self.embed_enc[:,:,t]\n",
        "            c_next = enc_memory[t]\n",
        "            c_prev = enc_memory[t-1]\n",
        "            a_next = enc_hidden[t]\n",
        "            a_prev = enc_hidden[t-1]\n",
        "            n_a = self.hidden\n",
        "   \n",
        "            \n",
        "            da = self.daa_dec                \n",
        "            dc = self.dcc_dec\n",
        "            dot = da*np.tanh(c_next)*ot*(1 - ot)\n",
        "            dcct = (dc*it + da*ot*(1- (np.tanh(c_next)**2))*it)*(1-cct**2)\n",
        "            dit = (dc*cct + da*ot*(1- (np.tanh(c_next)**2))*cct)*it*(1 - it)\n",
        "            dft = (dc*c_prev + da*ot*(1- (np.tanh(c_next)**2))*c_prev)*ft*(1 - ft)\n",
        "\n",
        "            self.dwf_enc += dft@(np.concatenate((a_prev , xt))).T\n",
        "            self.dwi_enc += dit@(np.concatenate((a_prev , xt))).T\n",
        "            self.dwc_enc += dcct@(np.concatenate((a_prev , xt))).T\n",
        "            self.dwo_enc += dot@(np.concatenate((a_prev , xt))).T\n",
        "            self.dbf_enc += np.sum(dft , axis = 1 , keepdims = True)\n",
        "            self.dbi_enc += np.sum(dit , axis = 1 , keepdims = True)\n",
        "            self.dbc_enc += np.sum(dcct , axis = 1 , keepdims = True)\n",
        "            self.dbo_enc += np.sum(dot , axis = 1 , keepdims = True)\n",
        "\n",
        "            self.daa_enc = self.wf_enc[:,:n_a].T@dft + self.wi_enc[:,:n_a].T@dit + self.wc_enc[:,:n_a].T@dcct + self.wo_enc[:,:n_a].T@dot\n",
        "            self.dcc_enc = dc*ft + da*ot*(1-(np.tanh(c_next)**2))*ft\n",
        "        pass\n",
        "\n",
        "    def update_params(self):\n",
        "\n",
        "        for d in [self.dwf_dec, self.dwi_dec, self.dwo_dec, self.dwc_dec, self.dwy_dec, self.dbf_dec, self.dbi_dec, self.dbo_dec, self.dbc_dec, self.dby_dec, self.dwf_enc, self.dwi_enc, self.dwo_enc, self.dwc_enc, self.dbf_enc, self.dbi_enc, self.dbo_enc, self.dbc_enc ]:\n",
        "            np.clip(d, -1, 1, out=d)                  #Gradient clipping to avoid exploding gradients\n",
        "        \n",
        "        #Updating Decoder Parameters\n",
        "        self.wf_dec -= self.lr*self.dwf_dec\n",
        "        self.wi_dec -= self.lr*self.dwi_dec\n",
        "        self.wo_dec -= self.lr*self.dwo_dec\n",
        "        self.wc_dec -= self.lr*self.dwc_dec\n",
        "        self.wy_dec -= self.lr*self.dwy_dec\n",
        "\n",
        "        self.bf_dec -= self.lr*self.dbf_dec\n",
        "        self.bi_dec -= self.lr*self.dbi_dec\n",
        "        self.bo_dec -= self.lr*self.dbo_dec\n",
        "        self.bc_dec -= self.lr*self.dbc_dec\n",
        "        self.by_dec -= self.lr*self.dby_dec\n",
        "\n",
        "\n",
        "        #Updating Encoder Parameter\n",
        "        self.wf_enc -= self.lr*self.dwf_enc\n",
        "        self.wi_enc -= self.lr*self.dwi_enc\n",
        "        self.wo_enc -= self.lr*self.dwo_enc\n",
        "        self.wc_enc -= self.lr*self.dwc_enc\n",
        "      \n",
        "        self.bf_enc -= self.lr*self.dbf_enc\n",
        "        self.bi_enc -= self.lr*self.dbi_enc\n",
        "        self.bo_enc -= self.lr*self.dbo_enc\n",
        "        self.bc_enc -= self.lr*self.dbc_enc\n",
        "    \n",
        "\n",
        "    def saveweight(model):\n",
        "    #save the weights \n",
        "      params= model\n",
        "      with open('weightRNN.dat','wb') as f:\n",
        "          pickle.dump(params,f)\n",
        "      print('weights saved successfully')\n",
        "      pass\n",
        "\n",
        "    def load_weights(model):\n",
        "      with open('weightRNN.dat','rb') as f:\n",
        "          weights = pickle.load(f)\n",
        "      return weights\n",
        "\n",
        "    def fit(X, Y, model):\n",
        "    #Training the model\n",
        "      costs = []\n",
        "      for epoch in range(model.epoch):\n",
        "          #Encoder Hidden State is captured after running Encoder Model\n",
        "          enc_hidden, enc_memory, enc_ft, enc_it, enc_ot, enc_cct = model.encoder_forward() \n",
        "          #Last Encoder hidden state and Memory cell is passed as context vector\n",
        "          dec_hidden, dec_memory, dec_output, dec_ft, dec_it, dec_ot, dec_cct = model.decoder_forward(enc_hidden[model.length_enc-1], enc_memory[model.length_enc-1] ) \n",
        "          #Compute Loss\n",
        "          loss = model.loss(dec_output, Y)   \n",
        "          #Print Loss\n",
        "          if (epoch%5 ==0):\n",
        "            costs.append(loss)\n",
        "            print('loss: ', loss)\n",
        "          #Backpropagation through time     \n",
        "          model.decoder_backward(dec_output, dec_hidden, dec_memory, Y , dec_ft, dec_it, dec_ot, dec_cct)  \n",
        "\n",
        "          model.encoder_backward(enc_hidden , enc_memory, enc_ft, enc_it, enc_ot, enc_cct)\n",
        "          #Gradient Descent = updation of parameters\n",
        "          model.update_params()\n",
        "\n",
        "      #Plot the cost  \n",
        "      plt.plot(np.squeeze(costs))\n",
        "      plt.ylabel('Cost')\n",
        "      plt.xlabel('Iterations (per 5)')\n",
        "      plt.title(\"Trainingloss\")\n",
        "      plt.show()\n",
        "      #Save Weights After Training\n",
        "      model.saveweight()\n",
        "      pass\n",
        "\n",
        "    #Sampling Function to Predict Traanslation   \n",
        "    def decoder_sampling (self, enc_hidden, enc_memory, idx_sos, idx_eos ) :\n",
        "        \n",
        "        def sigmoid(x):\n",
        "          return 1 / (1 + np.exp(-x))\n",
        "\n",
        "        def softmax(x):\n",
        "          e_x = np.exp(x - np.max(x))\n",
        "          return e_x / e_x.sum(axis=0)\n",
        "\n",
        "        #Forward prop in Decoder\n",
        "        dec_input = {}\n",
        "        dec_hidden = {}\n",
        "        dec_memory = {}\n",
        "\n",
        "        y_pred = {}\n",
        "        dec_output = {}\n",
        "\n",
        "        indices = []\n",
        "        \n",
        "        a_next = enc_hidden\n",
        "        c_next = enc_memory\n",
        "        dec_hidden[-1] = enc_hidden\n",
        "        dec_memory[-1] = enc_memory\n",
        "        xt = np.zeros((self.ger_vocab , 1))      #Setting intial Input as <sos> token vector\n",
        "        xt[idx_sos-1,0] = 1\n",
        "        dec_input[-1] = xt\n",
        "\n",
        "        idx = -1                                 #Initialize Index value\n",
        "\n",
        "        t=0                                      #Counter\n",
        "\n",
        "        while (idx+1 != idx_eos and t != 8) :    #Sample Until <eos> is reached or 8 words are sampled\n",
        "          concat = np.concatenate((a_next , xt))\n",
        "          ft = sigmoid(self.wf_dec @ concat + self.bf_dec )\n",
        "          it = sigmoid(self.wi_dec @ concat + self.bi_dec )\n",
        "          ot = sigmoid(self.wo_dec @ concat + self.bo_dec )\n",
        "          cct = np.tanh(self.wc_dec @ concat + self.bc_dec )\n",
        "\n",
        "          c_next = ((ft*c_next) + (it*cct))\n",
        "          a_next = ot*(np.tanh(c_next))\n",
        "\n",
        "          y_pred = softmax(self.wy_dec @a_next + self.by_dec)\n",
        "\n",
        "          output = np.zeros((self.ger_vocab , 1)) \n",
        "          np.random.seed(t)\n",
        "          idx = np.random.choice(list(range(self.ger_vocab)), p=y_pred[:,0].ravel())\n",
        "\n",
        "          indices.append(idx+1)\n",
        "          output[idx,0] = 1\n",
        "          xt = output\n",
        "          dec_input[t] = xt\n",
        "          dec_output[t] = output\n",
        "\n",
        "          t += 1\n",
        "\n",
        "        return indices                            #Return Output Indices\n",
        "\n",
        "    def predict(idx_sos, idx_eos, embed , eng_length, dummy):\n",
        "\n",
        "      model = dummy.load_weights()  #load the weights\n",
        "\n",
        "      model.embed_enc = embed\n",
        "      model.length_enc = eng_length\n",
        "      enc_hidden, enc_memory, enc_ft, enc_it, enc_ot, enc_cct = model.encoder_forward()\n",
        "\n",
        "      indices = model.decoder_sampling(enc_hidden[model.length_enc-1], enc_memory[model.length_enc-1], idx_sos, idx_eos)\n",
        "\n",
        "      return indices      \n",
        "\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRvWYT5TUjuT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "42a93381-baad-45dd-8b10-fc5e2eabb7eb"
      },
      "source": [
        "r = LSTM(Xenc_train, Xdec_train, vocab_eng_size, vocab_german_size, 400 , 0.0005 , 32, 50, max_eng_length , max_german_length)\n",
        "\n",
        "LSTM.fit(Xenc_train, Ydec_train, r)\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss:  23.763060933055403\n",
            "loss:  23.750424696283403\n",
            "loss:  23.73620503467814\n",
            "loss:  23.718966975954153\n",
            "loss:  23.696909539744915\n",
            "loss:  23.668531724502817\n",
            "loss:  23.632097052103205\n",
            "loss:  23.585437726498082\n",
            "loss:  23.525705171141936\n",
            "loss:  23.44942622020901\n",
            "loss:  23.35246208008086\n",
            "loss:  23.22996602868623\n",
            "loss:  23.076503553000492\n",
            "loss:  22.8866270389977\n",
            "loss:  22.655622006270494\n",
            "loss:  22.38138482570971\n",
            "loss:  22.065360524575013\n",
            "loss:  21.713553833878617\n",
            "loss:  21.33594639405843\n",
            "loss:  20.943727797262675\n",
            "loss:  20.548421695383382\n",
            "loss:  20.161116322199323\n",
            "loss:  19.7901169639208\n",
            "loss:  19.446674260531356\n",
            "loss:  19.132827796102195\n",
            "loss:  18.839616459842087\n",
            "loss:  18.565488298192765\n",
            "loss:  18.304106770106245\n",
            "loss:  18.063386322296473\n",
            "loss:  17.83565010343653\n",
            "loss:  17.62721041409125\n",
            "loss:  17.433688511653493\n",
            "loss:  17.249523145885746\n",
            "loss:  17.07470309236455\n",
            "loss:  16.910128978747416\n",
            "loss:  16.757086507921336\n",
            "loss:  16.616975991165553\n",
            "loss:  16.491145513629757\n",
            "loss:  16.42477176796495\n",
            "loss:  16.395612952417256\n",
            "loss:  16.37004529835998\n",
            "loss:  16.347658757103066\n",
            "loss:  16.32817557456613\n",
            "loss:  16.311322087982894\n",
            "loss:  16.29673637494098\n",
            "loss:  16.284163237576646\n",
            "loss:  16.273368875247165\n",
            "loss:  16.26345760489734\n",
            "loss:  16.247546756066125\n",
            "loss:  16.230195655911437\n",
            "loss:  16.21534336787862\n",
            "loss:  16.20304470931725\n",
            "loss:  16.1931071167457\n",
            "loss:  16.18516276595253\n",
            "loss:  16.178840273447985\n",
            "loss:  16.17385471406639\n",
            "loss:  16.16994560541838\n",
            "loss:  16.166864257386695\n",
            "loss:  16.16437855472681\n",
            "loss:  16.162279476899098\n",
            "loss:  16.160490009181252\n",
            "loss:  16.1589965206294\n",
            "loss:  16.157720401905557\n",
            "loss:  16.156614512694876\n",
            "loss:  16.155664561131708\n",
            "loss:  16.1548559182176\n",
            "loss:  16.15417860827234\n",
            "loss:  16.153617641128196\n",
            "loss:  16.15315021416984\n",
            "loss:  16.152756601694648\n",
            "loss:  16.152431830111013\n",
            "loss:  16.152168263751\n",
            "loss:  16.15194538955862\n",
            "loss:  16.151740883994208\n",
            "loss:  16.151550162888096\n",
            "loss:  16.15137611556944\n",
            "loss:  16.151220326449966\n",
            "loss:  16.151091744689584\n",
            "loss:  16.15098992751023\n",
            "loss:  16.150919308317683\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "weights saved successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvazRkAFv4HB"
      },
      "source": [
        ""
      ],
      "execution_count": 47,
      "outputs": []
    }
  ]
}